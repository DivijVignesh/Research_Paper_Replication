
### **Paper 1: Vision Transformer/CNN Block Analysis**  
*Investigating Transfer Learning Capabilities of Vision Transformers and CNNs by Fine-Tuning a Single Trainable Block*  
**Focus**:  
- Comparative transfer learning efficiency between ViTs and CNNs  
- Single-block fine-tuning paradigm analysis  
- Architectural alignment challenges in vision models  

### **Paper 2: DistilBERT Knowledge Distillation**  
*DistilBERT: Smaller, Faster, Cheaper and Lighter*  
**Focus**:  
- Language model compression via distillation  
- Cross-architecture knowledge transfer  
- Resource-efficient NLP deployment  



